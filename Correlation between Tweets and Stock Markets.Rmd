---
title: "Correlation between Tweets and Stock Markets"
author: "Amy Lee"
date: "8/13/2021"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Used Packages

```{r message=FALSE, warning=FALSE}
library(easypackages)
libraries("dplyr",
          "janitor",
          "lubridate",
          "ggplot2",
          "tidyr"
          )
```
```{r message=FALSE, warning=FALSE, include=FALSE, echo=FALSE}
mymachine="/Users/jungsoolee/Documents/Git/Effect\ of\ tweets\ on\ stock\ prices"
```

# Introduction

While I was looking for an interesting project to work on, I came across a Kaggle post called"Tweets about the Top Companies from 2015 and 2020".

(link : https://www.kaggle.com/omermetinn/tweets-about-the-top-companies-from-2015-to-2020)

It includes three data sets.
  
```{r message=FALSE, warning=FALSE}
twt_to_cmpy=read.csv(paste(mymachine,'/archive/Company_Tweet.csv', sep=""))
stk=read.csv(paste(mymachine,'/archive/CompanyValues.csv', sep=''))
twt=read.csv(paste(mymachine,'/archive/Tweet.csv', sep=''))
```

Using these, I intend to analyze the correlation between Tweets and company's stock prices.


## Exploratory Analysis

###1. Datasets
####a) Company_Tweet Dataset

```{r results = 'hold'}
str(twt_to_cmpy)
paste("number of distinct tweets =", n_distinct(twt_to_cmpy$tweet_id))
paste("company being referred =", paste(unique(twt_to_cmpy$ticker_symbol), collapse=", "))
paste("number of NA or blanks in tweet_id =", sum(is.na(twt_to_cmpy$tweet_id),nrow(twt_to_cmpy[twt_to_cmpy$tweet_id=="",])))
paste("number of NA or blanks in ticker_symbol =", sum(is.na(twt_to_cmpy$ticker_symbol),nrow(twt_to_cmpy[twt_to_cmpy$ticker_symbol=="",])))
```

This data set contains two columns.

And based on the setup, it seems to show which tweet posts are affecting which company.

There isn't much cleaning to do on this data, but it seems like there are duplicated tweet_id. (distinct number of tweets less than the total row).

Does this mean tweets can be related to more than one company at a time?

Let's find out.

```{r}
tmp<- twt_to_cmpy%>%
  group_by(tweet_id) %>%
  summarize(Referred_Company_Count_per_Tweet=n_distinct(ticker_symbol)) %>%
  count(Referred_Company_Count_per_Tweet) %>%
  rename(n_occurrence=n)
print(tmp)
paste("correct row number =", sum(tmp$Referred_Company_Count_per_Tweet*tmp$n_occurrence))
```
According to the above result, tweets can be related to multiple companies at a time. Also, the data set should have `r format(sum(tmp$Referred_Company_Count_per_Tweet*tmp$n_occurrence), big.mark=",")` rows while the table actually has `r format(nrow(twt_to_cmpy),big.mark=",")`.
This means some rows are duplicated and that needs to be eliminated.

```{r}
twt_to_cmpy<-unique(twt_to_cmpy)
nrow(twt_to_cmpy)
```



####b) CompanyValues Dataset
```{r result=hold}
str(stk)
stk$day_date=as.Date(stk$day_date)
paste("Date Range = min:", min(stk$day_date),"max:", max(stk$day_date))
paste("companies included in the data set =", paste(unique(stk$ticker_symbol), collapse=", "))
paste("number of NA or blanks in ticker_symbol =", sum(is.na(stk$ticker_symbol),nrow(stk[stk$ticker_symbol=="",])))
paste("number of NA in day_date =", sum(is.na(stk$day_date)))
paste("number of NA or blanks in close_value =", sum(is.na(stk$close_value),nrow(stk[stk$close_value=="",])))
paste("number of NA or blanks in open_value =", sum(is.na(stk$open_value),nrow(stk[stk$open_value=="",])))
paste("number of NA or blanks in high_value =", sum(is.na(stk$high_value),nrow(stk[stk$high_value=="",])))
paste("number of NA or blanks in low_value =", sum(is.na(stk$low_value),nrow(stk[stk$low_value=="",])))
paste("number of NA or blanks in volume =", sum(is.na(stk$volume),nrow(stk[stk$volume=="",])))
```

Looking at the structure of the data set, it doesn't seem to require much cleaning either.

But just to make sure, I will quickly check if the data includes stock info for every stocks each day.

```{r}
stk%>%
  group_by(day_date)%>%
  summarize(daily_recorded_companies=n_distinct(ticker_symbol))%>%
  arrange(daily_recorded_companies, descending=FALSE)%>%
  count(daily_recorded_companies)%>%
  rename(n_occurrence=n)

```
It seems like there are days where some companies' stock data is not recorded.

Let's take a deeper look into it.

```{r}
stk%>%
  group_by(ticker_symbol)%>%
  summarize(min_date=min(day_date), max_date=max(day_date))

stk%>%
  group_by(day_date)%>%
  summarize(daily_recorded_companies=n_distinct(ticker_symbol))%>%
  group_by(daily_recorded_companies)%>%
  summarize(min_date=min(day_date), max_date=max(day_date))

```

GOOG and TSLA data starts on different days.

But other than that, the it seems to be continuous through out the days.

####c) Tweet Dataset

```{r result=hold}
str(twt)
twt$post_datetime=as_datetime(twt$post_date)
twt$post_date=as_date(twt$post_datetime)
paste("Date Range = min:", min(twt$post_date),"max:", max(twt$post_date))
paste("number of NA or blanks in tweet_id =", sum(is.na(twt$tweet_id),nrow(twt[twt$tweet_id=="",])))
paste("number of NA or blanks in writer =", sum(is.na(twt$writer),nrow(twt[twt$writer=="",])))
paste("number of NA in post_date =", sum(is.na(twt$post_date)))
paste("number of NA or blanks in body =", sum(is.na(twt$body),nrow(twt[twt$body=="",])))
paste("number of NA or blanks in comment_num =", sum(is.na(twt$comment_num),nrow(twt[twt$comment_num=="",])))
paste("number of NA or blanks in retweet_num =", sum(is.na(twt$retweet_num),nrow(twt[twt$retweet_num=="",])))
paste("number of NA or blanks in like_num =", sum(is.na(twt$like_num),nrow(twt[twt$like_num=="",])))
paste("number of NA in post_datetime =", sum(is.na(twt$post_datetime)))
```

This data set does not have any NA values either. But, the record starts from 2015-01-01 to 2019-12-31.

Therefore, I will use the CompanyValue in the same time frame only.

```{r}
stk<-stk%>%
  filter(between(day_date, as.Date("2015-01-01"), as.Date("2019-12-31")))
```

First, I'd like to see how distributed "comment_num","retweet_num" and "like_num" values are.

```{r results='hold'}
ggplot(twt, aes(x=comment_num)) +
  geom_histogram(binwidth=20, color="gray", fill="gray") +
  stat_bin(binwidth=20, geom="text", aes(label=format(..count.., big.mark=",")), angle=75, size=3, hjust=0, vjust=0) +  
  scale_y_continuous(limits=c(0,4300000), labels = function(x) format(x, big.mark=",", scientific = FALSE)) +
  labs(title="Distribution of comment_num", y="frequency",x="comment_num") +
  theme(plot.title=element_text(hjust=0.5, size=18, face="bold"))

ggplot(twt, aes(x=retweet_num)) +
  geom_histogram(binwidth=20, color="gray", fill="gray") +
  stat_bin(binwidth=20, geom="text", aes(label=format(..count.., big.mark=",")), angle=75, size=3, hjust=0, vjust=0) +  
  scale_y_continuous(limits=c(0,4300000), labels = function(x) format(x, big.mark=",", scientific = FALSE)) +
  labs(title="Distribution of retweet_num", y="frequency",x="retweet_num") +
  theme(plot.title=element_text(hjust=0.5, size=18, face="bold"))
  
ggplot(twt, aes(x=like_num)) +
  geom_histogram(binwidth=20, color="gray", fill="gray") +
  stat_bin(binwidth=20, geom="text", aes(label=format(..count.., big.mark=",")), angle=75, size=3, hjust=0, vjust=0) +  
  scale_y_continuous(limits=c(0,4300000), labels = function(x) format(x, big.mark=",", scientific = FALSE)) +
  labs(title="Distribution of like_num", y="frequency",x="like_num") +
  theme(plot.title=element_text(hjust=0.5, size=18, face="bold"))

```

So, most of the tweets don't have any comments, likes, or hasn't been retweeted.

Let's see the ratio of 0 to non-zero values of these three columns.

```{r}
twt<-twt%>%
  mutate(yesno_comment=if_else(comment_num==0,"no_value","yes_value"),
         yesno_retweet=if_else(retweet_num==0,"no_value","yes_value"),
         yesno_like=if_else(like_num==0,"no_value","yes_value"))
twt%>%
  select(yesno_comment, yesno_retweet, yesno_like) %>%
  rename(comment=yesno_comment, retweet=yesno_retweet, like=yesno_like)%>%
  gather(key=type,value=yesno_value)%>%
  group_by(type, yesno_value) %>%
  summarize(p=sprintf('%.1f%%',n()*100/nrow(twt)), .groups="drop")%>%
  pivot_wider(id_cols="yesno_value", names_from = "type", values_from=p)
```

From this result, we can see that 85.6% of the whole tweets do not have any comments, 66.8% don't have likes and 83.3% haven't been retweeted.

Now, I would like to see the hourly distribution of the tweet update.

I will like to check if the posted hour affect whether the tweet will get likes, comments or retweeted. 

Let's define tweets that has been liked, commented or retweeted as having awareness.
```{r}
twt.p1<-twt%>%
  mutate(aware=if_else((comment_num+like_num+retweet_num)==0,"no_awareness","yes_awareness"))%>%
  group_by(hour(post_datetime), aware)%>%
  summarize(tweet_ct=n(), .groups='drop')%>%
  rename(post_hour="hour(post_datetime)")
  
twt.p1<-twt.p1%>%
  mutate(aware="total")%>%
  group_by(post_hour, aware)%>%
  summarize(tweet_ct=sum(tweet_ct), .groups="drop")%>%
  rbind(twt.p1)

p1<-ggplot(twt.p1, aes(x=post_hour, y=tweet_ct, group=aware))+
  geom_line(aes(color=aware, linetype=aware), size=0.6)+
  geom_point(aes(color=aware, shape=aware), size=2)+
  scale_color_manual(values=c(total="#9B59B6", no_awareness="#E74C3C", yes_awareness="#3498DB"))+
  scale_linetype_manual(values=c(total="solid", no_awareness="dotted", yes_awareness="dotted"))+
  scale_shape_manual(values=c(total=19, no_awareness=15, yes_awareness=17))+
  theme(
    panel.background=element_rect(fill="white", color="gray", size=0.7),
    panel.grid.major.y=element_line(size=0.25, color="gray", linetype="dotted"),
    panel.grid.major.x=element_line(size=0.25, color="gray", linetype="dotted"),
    panel.grid.minor.x=element_line(size=0.25, color="gray", linetype="dotted"),
    axis.title=element_text(size=10),
    plot.title=element_text(size=14, hjust=0.5, face="bold"),
    legend.title=element_blank(),
    legend.justification = "top"
  ) +
  scale_x_continuous(minor_breaks=c(0:23)) +
  scale_y_continuous(labels=function(x) {format(x,big.mark = ",", scientific = FALSE)}, n.breaks = 10) +
  labs(title="Hourly Distribution of Tweets by Awareness", x="Hour", y="Number of Tweets")
p1
```
According to the graph above, all three lines share the same shape.

This signifies that posted hour is not correlated to awareness.

This hypothesis can be tested by performing the correlation test (calculating Pearson's correlation coefficient and testing the hypothesis of significant correlation)

```{r}
cor1<-twt%>%
  mutate(post_hour=hour(post_datetime),
         aware=if_else(comment_num+like_num+retweet_num>0,0,1))%>%
  select(post_hour, aware)

summary(cor1)

cor.test(cor1$post_hour, cor1$aware, method="spearman")
ggplot(cor1, aes(x=aware, y=post_hour))+
  geom_boxplot()
```












if it is correlated whether the tweet has been commented, liked, or retweeted.















Next, I will perform general analysis on the content of the tweets.
```{r}
twt$body_len=sapply(
  lapply(
    strsplit(twt$body," ") #splitting strings by " "
         , function(x){x[!x==""]}) #using lapply to delete blank elements
  , length) #counting the number of elements in the list.
summary(twt$body_len)

a<-twt%>%
  arrange(desc(twt$body_len))%>%
  head(100)
```














```{r}
  #group_by(Referred_Company_Count) %>%
  #summarize(n_occurrence=count(Referred_Company_Count))
  #arrange(desc(Referred_Company_Count)) %>% #sort(Referred_Company_Count, decreasing=TRUE) %>%
  #head(20)
test<-twt%>%
  filter(post_date=="")
```









